{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Phenomena\n",
    "\n",
    "Quantum effects can be very counterintuitive, in some sense our intuition is not built for a quantum world because the macroscopic world looks very classical. An excellent example of quantum magic is quantum tunneling:\n",
    "\n",
    "Imagine a particle travelling along a potential energy surface $V(x)$ (time independent) with some kinetic energy ($T$). The total energy ($E$) is simply:\n",
    "\n",
    "$$ E = T + V $$\n",
    "\n",
    "If there is a potential energy barrier which is higher than the total energy $E$ of our particle, then classically the particle will never get past the barrier.\n",
    "\n",
    "However, in the quantum regime, we have to consider the wave-like nature of our particle. As you may remember, when a wave encounters a barrier, some of it will be reflected and some of it will be transmitted. This suggests that as long as the energy barrier is finitely high, the particle may in fact slip through.\n",
    "\n",
    "This combined with de Broglie's realization that all matter is a wave, brings us to a startling realization. Say we have a glass of water (ignoring effects like evaporation), if we wait long enough, the water molecules will tunnel through the glass (not jump over it). Don't try this at home though, chances are you'll see the heat death of the universe long before any molecules get through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually observe some tunneling by numerically solving Schrödinger's equation, just like in the `schroedinger_equation.ipynb` notebook, simply by choosing the potential energy function $V(x)$ carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 300\n",
    "L = 100\n",
    "w = 30\n",
    "\n",
    "def V(x):\n",
    "    n = len(x)\n",
    "    V = np.ones(n)*0\n",
    "    V[np.abs(x)<=w/2] = 0.05\n",
    "    return V\n",
    "\n",
    "eta = 1\n",
    "m = 1\n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0,L,num=N) - L/2\n",
    "a = X[1] - X[0] # grid spacing\n",
    "\n",
    "t = -eta**2 / (2 * m * a**2)\n",
    "eps = -2*t + q * V(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(X, V(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('V(x)')\n",
    "plt.title('Potential Energy Surface')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we the hamiltonian for our discretized system, and solve for the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = t*np.eye(N, k=-1) + eps*np.eye(N) + t*np.eye(N, k=1) # discretized hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, vecs = np.linalg.eig(H)\n",
    "vals.shape, vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = np.argsort(vals)\n",
    "vals, vecs = vals[order], vecs[:, order]\n",
    "vecs = vecs.T\n",
    "vecs /= np.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, sharex=True, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.title('Wavefunctions of a particle in a box, with a barrier')\n",
    "for i, (ax, psi, E) in enumerate(zip(axes, vecs, vals)):\n",
    "    plt.sca(ax)\n",
    "    plt.ylabel('Psi {}'.format(i))\n",
    "    plt.plot(X, psi)\n",
    "    #plt.fill_between(X, psi, alpha=0.3)\n",
    "    print('E_{} = {:.4f}'.format(i, E))\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the computed energies for the first 8 levels and compare them to the height of our potential energy barrier. Remember that the eigenvalues of our hamiltonian are the total energies $E$ of each state.\n",
    "\n",
    "Just as before, we'll use our wavefunctions to predict the outcomes of measurements of the positions of our particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.abs(vecs)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, sharex=True, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.title('Position distributions of a particle in a box with barrier')\n",
    "for i, (ax, pᵢ) in enumerate(zip(axes, p)):\n",
    "    plt.sca(ax)\n",
    "    plt.ylabel('p_{}(x)'.format(i))\n",
    "    plt.plot(X, pᵢ)\n",
    "    plt.fill_between(X, pᵢ, alpha=0.3)\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, what do you notice about the distributions of the even and odd solutions we found?\n",
    "\n",
    "For a particle in a box (considered in `schroedinger equation.ipynb`) we saw that our solutions for the wavefunction alternated between being even and odd (remember what it means for a function to be even/odd?). Now our potential is very similar, except that there is a barrier in the middle, resulting in a well on either side of the barrier. If we look at our wavefunctions for only a single well, they resemble the solutions to the particle in the box.\n",
    "\n",
    "If we think of our system as being two particles in a box next to each other, then what differentiates the ground state from the first excited state in our system? Looking at the plotted wavefunctions, we see in the ground state our two \"imaginary\" particles are in phase, while in the first excited state the particles are out of phase. In fact, this is related to electrons of different atoms interacting to form bonding and antibonding orbitals in molecules (for further reading). For now, just note the similarities between this system and the particle in the box, and understand the resulting trends in the energy spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the probability of measuring our particle in the barrier for each energy level using our probability density functions.\n",
    "To find the probability of some event $B$, we integrate our probability density function over all states in $B$. Since we discretized our probability density function, the integral turns into a sum using our grid spacing $a$.\n",
    "\n",
    "$$ \\mathrm{Prob}(B) = \\int_{x \\in B} p(x) dx = a \\sum_{x \\in B} p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = V(X) != 0\n",
    "\n",
    "barrier = a * p[:, B].sum(-1)\n",
    "\n",
    "num = 8\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.title('Energy')\n",
    "plt.plot(np.arange(num), vals[:num], color='b', ls='', marker='x')\n",
    "plt.plot(np.arange(num), np.ones(num)*V(X)[B].mean(), ls='--')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Energy (arb units)')\n",
    "#ax1.tick_params('y', colors='b')\n",
    "\n",
    "#ax2 = ax1.twinx()\n",
    "plt.sca(ax2)\n",
    "plt.title('Probability')\n",
    "plt.plot(np.arange(num), barrier[:num], color='r', ls='', marker='o')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Probability of being in barrier')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print('Level | Energy | Prob(B)')\n",
    "print('------+--------+--------')\n",
    "for i, (E, prob_barrier) in enumerate(zip(vals[:8], barrier)):\n",
    "    print('{:6}|  {:.4f}|  {:.4f}'.format(i, E, prob_barrier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reflect on the results a little bit. Specifically, look at the fourth excited state (level 4), note that the total energy of this state is significantly lower (about $0.3$) than our barrier ($0.5$), yet the probability of finding the particle in the barrier is over 5%, which means we only have to repeat this experiment around 20 times before we expect to find the particle in the barrier. \n",
    "\n",
    "If we know the particle is in the fourth excited state, and we find it inside the barrier, then we know the total energy $E \\approx 0.3$ and the potential energy $V = 0.5$. This would suggest the kinetic energy is negative. Recall the definition of the kinetic energy $T = \\frac{p^2}{2m}$, the mass is certainly not negative, and we can't make sense of an imaginary momentum - so how can we make sense of this?\n",
    "\n",
    "We can conclude that in a quantum world, we simply can't know the kinetic energy when we've measured the position. In other words, these two measurements don't commute. This is a very conceptual derivation of the uncertainty principle, which is usually first introduced between position and momentum, but as kinetic energy is related to momentum, the same principle holds for position and kinetic energy. \n",
    "\n",
    "More formally, given two operators $\\mathrm{A}$ and $\\mathrm{B}$, their commutation relation is given by operator $ \\left[\\mathrm{A}, \\mathrm{B} \\right] $:\n",
    "\n",
    "$$ \\left[\\mathrm{A}, \\mathrm{B} \\right] = \\mathrm{A}\\mathrm{B} - \\mathrm{B}\\mathrm{A} $$\n",
    "\n",
    "$\\mathrm{A}$ and $\\mathrm{B}$ are said to commute if and only if their commutation relation is zero, $\\left[\\mathrm{A}, \\mathrm{B} \\right] = 0$.\n",
    "\n",
    "This is slightly sloppy notation, as operators without functions to act on do no make well formed expressions, so strictly speaking, we should say $\\mathrm{A}$ and $\\mathrm{B}$ commute if and only if $\\left[\\mathrm{A}, \\mathrm{B} \\right]=0$ for any $\\phi$\n",
    "\n",
    "$$ (\\mathrm{A}\\mathrm{B} - \\mathrm{B}\\mathrm{A}) \\phi = \\left[\\mathrm{A}, \\mathrm{B} \\right] \\phi = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Position vs. Kinetic Energy Commutation\n",
    "\n",
    "Show that the position operator and the kinetic energy operator do not commute and derive their commutation relation, given their definitions below.\n",
    "\n",
    "$$ \\hat{x} = x$$\n",
    "$$ \\hat{T} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} = \\frac{\\hat{p}^2}{2m} $$\n",
    "\n",
    "$$ [\\hat{x}, \\hat{T}] =~?$$\n",
    "\n",
    "Hint: a possibly useful property of commutation relations: $ \\left[\\mathrm{A}, \\mathrm{B}^2 \\right] = \\left[\\mathrm{A}, \\mathrm{B} \\right] \\mathrm{B} + \\mathrm{B} \\left[\\mathrm{A}, \\mathrm{B} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superposition\n",
    "\n",
    "Classically the state of a system can be defined in terms of observables, such as position and momentum. Incidentally, for any physical system, if you specify the positions and momenta of all degrees of freedom then you have fully specified the state. \n",
    "\n",
    "However, in quantum mechanics we treat the state of our system (the wavefunction) separately from the observables (which correspond to operators). With this formalism, the outcome of a measurement (applying an operator to the wavefunction) corresponds to an eigenvalue of the operator corresponding to that observable. Since the measurements we make must be real, it follows that all eigenvalues of operators for observables are real, so the operators must be hermitian.\n",
    "\n",
    "So far so good, if our state is in an eigenstate of some observable, then when we make the corresponding measurement, we'll get the corresponding eigenvalue as the outcome. However, the Hamiltonian is a linear operator, so any linear combination of solultions to the Schrödinger equation is also a valid solution. So how do we interpret linear combinations, aka a superposition, of eigenstates?\n",
    "\n",
    "Given some operator $\\mathbf{\\hat{O}}$ with eigenvalues $\\lambda_i$ and corresponding eigenstates $|\\phi_i \\rangle$. If $\\mathbf{\\hat{O}}$ is to correspond to an observable in the real world, it better be hermitian and unitary, which means the eigenstates $|\\phi_i \\rangle$ form an orthonormal bases:\n",
    "\n",
    "$$ \\langle \\phi_i | \\phi_j \\rangle = \\delta_{ij} $$\n",
    "\n",
    "where $ \\delta_{ij} $ is the Kronecker delta.\n",
    "\n",
    "Say we are in the state $|\\psi \\rangle = \\sum_i c_i | \\phi_i \\rangle$ for some coefficients $c_i$. First, a bit of linear algebra knowledge tells us we can write any state imaginable as a linear combination of eigenstates because the eigenstates form an orthonormal basis.\n",
    "\n",
    "Next, we can recognize that, since we want our state $|\\psi \\rangle$ to be normalized (wavefunctions are always normalized!), and since each of the eigenstates $|\\psi \\rangle$ are normalized that places a constraint on the coeffients $c_i$:\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = 1 $$\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = \\left( \\sum_i c_i^* \\langle \\phi_i | \\right)\\left( \\sum_j c_j | \\phi_j \\rangle \\right) $$\n",
    "\n",
    "By orthonormality the cross terms cancel out,\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = \\sum_i c_i^* c_i \\langle \\phi_i | \\phi_i \\rangle $$\n",
    "\n",
    "So, for our general state $|\\psi \\rangle$ to be normalized $c_i$ may be anything (including being complex) as long as the coeffients obey:\n",
    "\n",
    "$$ 1 = \\sum_i |c_i|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we dealt with stationary states (eigenstates of the hamiltonian), the time evolution of our system simply changed the overall phase of our state, but the time evolution of a superposition is more complicated.\n",
    "\n",
    "For each energy eigenstate $\\phi_j$ with corresponding eigenvalue $E_j$, we know the time evolution is given by $e^{-i\\frac{E_j}{\\hbar}t}$. When dealing with a linear combination of eigenstates, we can simply include the corresponding temporal component to each term in the linear combination to produce a valid solution of the time dependent Schrödinger equation.\n",
    "\n",
    "$$ | \\psi \\rangle = \\sum_j c_j | \\phi_j \\rangle \\rightarrow |\\Psi(x,t)\\rangle = \\sum_j c_j e^{-i\\frac{E_j}{\\hbar}t} | \\phi_j \\rangle $$\n",
    "\n",
    "where $|\\phi_j \\rangle$ and $E_j$ are the eigenstate/eigenvalue pairs of the hamiltonian.\n",
    "\n",
    "Now we can observe the time evolution of superposition states, once again using `matplotlib` animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "np.set_printoptions(precision=4, linewidth=110) # just to make printing arrays slightly nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve our favorite equation with the potential of our choice: for example, a harmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 300\n",
    "L = 100\n",
    "\n",
    "def V(x): # Return the potential energy given the position of each grid element\n",
    "    return 0.001 * x**2\n",
    "    #return 0 * x # In this case, we are setting the potential energy to zero.\n",
    "\n",
    "eta = 1\n",
    "m = 1\n",
    "q = 1\n",
    "\n",
    "X = np.linspace(0,L,num=N) - L/2\n",
    "a = X[1] - X[0] # grid spacing\n",
    "\n",
    "t = -eta**2 / (2 * m * a**2)\n",
    "eps = -2*t + q * V(X)\n",
    "\n",
    "# Define discretized hamiltonian\n",
    "H = t*np.eye(N, k=-1) + eps*np.eye(N) + t*np.eye(N, k=1)\n",
    "\n",
    "# Solve TISE to get wavefunctions and energies\n",
    "vals, vecs = np.linalg.eig(H)\n",
    "order = np.argsort(vals)\n",
    "vals, vecs = vals[order], vecs[:, order]\n",
    "vecs = vecs.T\n",
    "# Normalize wavefunction\n",
    "vecs /= np.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(X, V(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('V(x)')\n",
    "plt.title('Potential Energy Surface')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at arbitrary superpositions we have to reflect on how we solved the Schrödinger equation in the first place. As we discretizing space, we can't trust the computed high energy eigenstates/eigenvalues. Consequently, we should choose some cutoff (say 20), and only consider superpositions of states below that cutoff.\n",
    "\n",
    "After that you can specify what ever (unnormalized) superposition of levels you want in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = 20 # how many energy levels to trust from our computed results (probably shouldn't be more than 20 or so)\n",
    "psi = vecs[:cutoff].astype(np.complex64)\n",
    "E = vals[:cutoff]\n",
    "coeff = np.zeros(cutoff, dtype=np.complex64)\n",
    "\n",
    "# Here you can set the coefficients of the state to anything you want (don't worry about normalization yet)\n",
    "\n",
    "coeff[4] = 1\n",
    "coeff[0] = 1j\n",
    "coeff[3] = -2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never forget to normalize the wavefunction - as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    norm = (np.abs(coeff)**2).sum()\n",
    "    assert norm > 0\n",
    "    coeff /= np.sqrt(norm)\n",
    "except AssertionError:\n",
    "    print('Error: Norm is zero, you must set the coeffients to nonzero')\n",
    "    \n",
    "print('Coeffients:', coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we define an `evolve` function for the matplotlib animation to compute the wavefunction at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evolve(coeff, psi, E, timestep=5, num_step=300):\n",
    "    assert len(coeff) == len(psi) == len(E)\n",
    "    ts = np.arange(num_step)*timestep\n",
    "    t = 0\n",
    "    cnt = 0\n",
    "    while num_step is None or cnt < num_step:\n",
    "        c = coeff * np.exp(- 1j * E / eta * t)\n",
    "        cnt += 1\n",
    "        t += timestep\n",
    "        yield t, c @ psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 1\n",
    "num_step = 300\n",
    "\n",
    "if timestep is None:\n",
    "    av_E = (coeff)**2 @ E\n",
    "    timestep = 2*np.pi * eta / av_E / num_step\n",
    "    print('Using timestep: {:.4f}'.format(timestep))\n",
    "    \n",
    "evolution = evolve(coeff, psi, E, timestep=timestep, num_step=num_step)\n",
    "\n",
    "print('Real component in red, imaginary in blue')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "ax1.set_title('Wavefunction')\n",
    "ax1.set_ylabel('psi(x)')\n",
    "ax2.set_title('Probability Density')\n",
    "ax2.set_ylabel('p(x)')\n",
    "ax2.set_xlabel('x')\n",
    "wave = (coeff @ psi)\n",
    "wave_real, = ax1.plot(X, wave.real, c='b')\n",
    "wave_imag, = ax1.plot(X, wave.imag, c='r')\n",
    "p = np.abs(wave)**2\n",
    "dens, = ax2.plot(X, p, color='k')\n",
    "#ax2.fill_between(X, p, color='k', alpha=0.2)\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "lim = np.sqrt(p).max()+0.1\n",
    "ax1.set_ylim(-lim, lim)\n",
    "ax2.set_ylim(-1e-4, max(p)+0.02)\n",
    "\n",
    "def run(data):\n",
    "    # update the data\n",
    "    t, wave = data\n",
    "    \n",
    "    ax1.set_title('Wavefunction (t={:.2f})'.format(t))\n",
    "    \n",
    "    wave_real.set_data(X, wave.real)\n",
    "    wave_imag.set_data(X, wave.imag)\n",
    "\n",
    "    p = np.abs(wave)**2\n",
    "    dens.set_data(X, p)\n",
    "    \n",
    "    #fig.canvas.draw()\n",
    "    \n",
    "    return wave_real, wave_imag, dens\n",
    "\n",
    "ani = animation.FuncAnimation(fig, run, evolution, blit=False, interval=10, repeat=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Oscillating Superpositions\n",
    "\n",
    "Test a few different superpositions and observe how the wavefunction and, more importantly, the probability distribution over positions changes with time, then answer the questions below: (A short explanation/example for each question is fine)\n",
    "\n",
    "1. How does the behavior compare between:\n",
    "    1. Superpositions of only even states vs superpositions of only odd solutions?\n",
    "    2. Superpositions of only high energy (below the cutoff though) states vs only low energy eigenstates?\n",
    "    3. Superpositions with a single dominant term (one of the coefficients is significantly larger than all the others) vs fairly balanced superpositions?\n",
    "    4. Superpositions where each term has the same phase vs where terms has a significantly different phase.\n",
    "\n",
    "2. Give an example of a superposition that causes a large amount of probability density to oscillate back and forth uniformly (as one mass). In other words, what superposition produces a \"pseudo-classical\" oscillation in our harmonic well with as large of an amplitude you can achieve. (You don't have to find the optimal superposition, just one that looks good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement\n",
    "\n",
    "Since we can write any general state in terms of the eigenstates of a particular operator. Thinking about measurement becomes a little tricky with superpositions. The Copenhagen interpretation of quantum mechanics (which is most popular interpretation) posits that we can't directly measure a superposition of eigenstates. Instead, when we make a measurment of a particle in a superposition of eigenstates, the wavefunction \"collapses\" irreversibly to one of the eigenstates of our operator, and then the outcome we actually observe is the corresponding eigenvalue. Furthermore, the probability that our state $|\\psi \\rangle$ collapses to state $|\\phi_i \\rangle$ is given by $|c_i|^2$.\n",
    "\n",
    "So, in short the probability of measuring eigenvalue $\\lambda_i$ is given by:\n",
    "\n",
    "$$ \\mathrm{Prob}(\\lambda_i) = | \\langle \\phi_i | \\psi \\rangle |^2 $$\n",
    "\n",
    "This interpretation has massive physical and philosophical implications, which have freaked many of the greatest minds in the past and present. For example, Einstein who received the Nobel prize for his work in quantum mechanics vehemently opposed that any aspect of the universe could be nondeterministic. Nevertheless, the Copenhagen interpretation would suggest the outcome of an experiment can in fact be nondeterministic. Crazy, huh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, in quantum mechanics, we have to be a little more careful about what we mean by the position, momentum, or any other observable of our particle. Since measuring an arbitrary state $|\\psi \\rangle$ could result in one of infinitely many outcomes, one common way to think about measurment outcomes is expectation values. For state $|\\psi \\rangle$ the expectation value of some operator $\\mathbf{\\hat{O}}$ is given by:\n",
    "\n",
    "$$ \\langle \\mathbf{\\hat{O}} \\rangle = \\langle \\psi | \\mathbf{\\hat{O}} | \\psi \\rangle = \\sum_i |c_i|^2 \\lambda_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Expectation Values\n",
    "\n",
    "Given a particle in state $|\\chi \\rangle$, trapped in a harmonic potential ($V(x) = 0.001 x^2$) with parameters ($\\eta = 1$, $m = 1$, $q = 1$, $N = 300$, and $L = 100$, so $a \\approx \\frac{1}{3}$):\n",
    "\n",
    "$$ | \\chi \\rangle = 2 | \\phi_1 \\rangle + 6i | \\phi_2 \\rangle - 3| \\phi_5 \\rangle $$\n",
    "\n",
    "where $\\phi_i$ is the $i$th eigenstate of the hamiltonian ($\\phi_0$ is the ground state).\n",
    "\n",
    "First normalize $| \\chi \\rangle$, then compute the expectation value of the hamiltonian $\\langle \\mathbf{\\hat{H}} \\rangle$. Compare the result you get in theory and empirically by numerically solving the Schrödinger equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Changing basis\n",
    "\n",
    "As we saw earlier, the position and hamiltonian operators do not commute. However, their eigenstates form a basis over the same space, so we can write any eigenstate of the position operator as a linear combination of energy eigenstates.\n",
    "\n",
    "Unfortunately, since we are numerically approximating the true hamitonian and position operators of our system, the accuracy of high energy eigenstates we compute suffer from the numerical approximations, and therefore can't be trusted.\n",
    "\n",
    "Find the linear combination of the 20 lowest energy states of the particle in the harmonic potential above ($V(x) = 0.001 x^2$) which best approximates the position eigenstate for the particle being at $x=0$. (Using the parameters defined above: $\\eta = 1$, $m = 1$, $q = 1$, $N = 300$, and $L = 100$)\n",
    "\n",
    "Hint: Position eigenstates correspond to Dirac delta functions, eg. the eigenstate for a particle at $x=x_0$ is $\\phi(x) = \\delta(x-x_0)$, so what would a position eigenstate look like in our discretized system.\n",
    "\n",
    "1. After finding the coefficients, plot the superposition at t = 0. \n",
    "\n",
    "2. Plot the superposition as a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
